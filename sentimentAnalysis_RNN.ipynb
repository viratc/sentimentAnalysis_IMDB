{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import pyprind\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the Data file\n",
    "# Here we use the file(movie_data.csv) generated at the \n",
    "# beginning of sentimentAnalysis.ipynb\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting word occurrences\n",
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:08:29\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data: Separate all the words & count each word's\n",
    "# occurrence\n",
    "\n",
    "# To find unique words in this large data set, using \"sets\" is not very \n",
    "# efficient. Hence, we'll use Counter from collections package.\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "\n",
    "# Progress Bar to keep us updated\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Counting word occurrences')\n",
    "\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' '\n",
    "                    for c in review]).lower()\n",
    "    \n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', '.', ',', 'and', 'a']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping\n",
    "# Mapping each unique word to a integer\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "word_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n",
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:03\n"
     ]
    }
   ],
   "source": [
    "word_to_int = {word: i for i, word in enumerate(word_counts, 1)}\n",
    "mapped_reviews = []\n",
    "\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    \n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15,\n",
       "  5646,\n",
       "  3,\n",
       "  1,\n",
       "  2160,\n",
       "  3977,\n",
       "  26959,\n",
       "  30,\n",
       "  4824,\n",
       "  1575,\n",
       "  29,\n",
       "  1133,\n",
       "  7,\n",
       "  1,\n",
       "  316,\n",
       "  19,\n",
       "  720,\n",
       "  1612,\n",
       "  6,\n",
       "  6653,\n",
       "  753,\n",
       "  3,\n",
       "  14781,\n",
       "  3,\n",
       "  8680,\n",
       "  2,\n",
       "  33,\n",
       "  1,\n",
       "  14782,\n",
       "  328,\n",
       "  3,\n",
       "  3612,\n",
       "  6,\n",
       "  2105,\n",
       "  3,\n",
       "  67,\n",
       "  22,\n",
       "  1947,\n",
       "  15,\n",
       "  1,\n",
       "  8755,\n",
       "  6,\n",
       "  54,\n",
       "  339,\n",
       "  4,\n",
       "  54,\n",
       "  602,\n",
       "  4712,\n",
       "  15343,\n",
       "  2,\n",
       "  1732,\n",
       "  19,\n",
       "  121,\n",
       "  170,\n",
       "  323,\n",
       "  3,\n",
       "  1,\n",
       "  565,\n",
       "  953,\n",
       "  25748,\n",
       "  30,\n",
       "  1464,\n",
       "  19622,\n",
       "  29,\n",
       "  3,\n",
       "  47,\n",
       "  10,\n",
       "  5,\n",
       "  1121,\n",
       "  1099,\n",
       "  1362,\n",
       "  18,\n",
       "  58,\n",
       "  3040,\n",
       "  15,\n",
       "  6002,\n",
       "  25,\n",
       "  44343,\n",
       "  15,\n",
       "  716,\n",
       "  2,\n",
       "  1544,\n",
       "  2,\n",
       "  5290,\n",
       "  3192,\n",
       "  4,\n",
       "  1670,\n",
       "  7,\n",
       "  21518,\n",
       "  3,\n",
       "  1103,\n",
       "  7,\n",
       "  3921,\n",
       "  1,\n",
       "  434,\n",
       "  26,\n",
       "  37,\n",
       "  1956,\n",
       "  1801,\n",
       "  2333,\n",
       "  30,\n",
       "  3679,\n",
       "  3294,\n",
       "  29,\n",
       "  26,\n",
       "  1,\n",
       "  1285,\n",
       "  6,\n",
       "  507,\n",
       "  5,\n",
       "  286,\n",
       "  2,\n",
       "  1,\n",
       "  5463,\n",
       "  10558,\n",
       "  4,\n",
       "  92,\n",
       "  34,\n",
       "  2568,\n",
       "  106,\n",
       "  3,\n",
       "  27,\n",
       "  26,\n",
       "  1,\n",
       "  1457,\n",
       "  6,\n",
       "  1,\n",
       "  5056,\n",
       "  1362,\n",
       "  1320,\n",
       "  8833,\n",
       "  30,\n",
       "  623,\n",
       "  12704,\n",
       "  29,\n",
       "  18,\n",
       "  22,\n",
       "  15,\n",
       "  2762,\n",
       "  6,\n",
       "  1,\n",
       "  3488,\n",
       "  15,\n",
       "  1,\n",
       "  1358,\n",
       "  8,\n",
       "  21,\n",
       "  3,\n",
       "  44,\n",
       "  1873,\n",
       "  1,\n",
       "  1795,\n",
       "  4,\n",
       "  5,\n",
       "  5000,\n",
       "  6,\n",
       "  661,\n",
       "  4,\n",
       "  308,\n",
       "  7,\n",
       "  999,\n",
       "  1,\n",
       "  602,\n",
       "  2,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  20,\n",
       "  602,\n",
       "  15,\n",
       "  14781,\n",
       "  20,\n",
       "  10,\n",
       "  5,\n",
       "  62,\n",
       "  255,\n",
       "  24,\n",
       "  3,\n",
       "  26,\n",
       "  1,\n",
       "  307,\n",
       "  76,\n",
       "  6,\n",
       "  5,\n",
       "  602,\n",
       "  6,\n",
       "  5,\n",
       "  3521,\n",
       "  170,\n",
       "  174,\n",
       "  260,\n",
       "  18,\n",
       "  22,\n",
       "  2373,\n",
       "  45,\n",
       "  5,\n",
       "  3041,\n",
       "  2160,\n",
       "  654,\n",
       "  412,\n",
       "  22,\n",
       "  5,\n",
       "  3129,\n",
       "  2,\n",
       "  1,\n",
       "  976,\n",
       "  4,\n",
       "  994,\n",
       "  244,\n",
       "  347,\n",
       "  79,\n",
       "  2413,\n",
       "  7,\n",
       "  999,\n",
       "  1,\n",
       "  602,\n",
       "  25,\n",
       "  65,\n",
       "  86,\n",
       "  1732,\n",
       "  170,\n",
       "  2,\n",
       "  207,\n",
       "  3,\n",
       "  5,\n",
       "  39815,\n",
       "  1362,\n",
       "  4,\n",
       "  6200,\n",
       "  63257,\n",
       "  15,\n",
       "  6002,\n",
       "  22,\n",
       "  495,\n",
       "  7,\n",
       "  24719,\n",
       "  99,\n",
       "  1,\n",
       "  4457,\n",
       "  830,\n",
       "  22,\n",
       "  2373,\n",
       "  2,\n",
       "  1,\n",
       "  926,\n",
       "  293,\n",
       "  1,\n",
       "  3488,\n",
       "  6,\n",
       "  953,\n",
       "  4,\n",
       "  1,\n",
       "  254,\n",
       "  498,\n",
       "  6,\n",
       "  3977,\n",
       "  15,\n",
       "  4253,\n",
       "  3,\n",
       "  27,\n",
       "  51,\n",
       "  10,\n",
       "  5,\n",
       "  588,\n",
       "  6,\n",
       "  1,\n",
       "  1430,\n",
       "  15,\n",
       "  1,\n",
       "  12170,\n",
       "  2,\n",
       "  71,\n",
       "  2071,\n",
       "  10,\n",
       "  1589,\n",
       "  2,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  436,\n",
       "  30,\n",
       "  3535,\n",
       "  29,\n",
       "  85,\n",
       "  34,\n",
       "  1309],\n",
       " [589,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  48,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  16,\n",
       "  77,\n",
       "  50,\n",
       "  7431,\n",
       "  7487,\n",
       "  4,\n",
       "  37,\n",
       "  679,\n",
       "  776,\n",
       "  184,\n",
       "  2826,\n",
       "  6,\n",
       "  427,\n",
       "  15,\n",
       "  37,\n",
       "  112,\n",
       "  2,\n",
       "  611,\n",
       "  58,\n",
       "  1649,\n",
       "  100,\n",
       "  26,\n",
       "  37,\n",
       "  1761,\n",
       "  3012,\n",
       "  376,\n",
       "  1711,\n",
       "  414,\n",
       "  4,\n",
       "  35,\n",
       "  95,\n",
       "  2098,\n",
       "  5,\n",
       "  146,\n",
       "  8080,\n",
       "  2,\n",
       "  27,\n",
       "  3,\n",
       "  8517,\n",
       "  10,\n",
       "  37,\n",
       "  18183,\n",
       "  2,\n",
       "  3959,\n",
       "  51063,\n",
       "  3,\n",
       "  17,\n",
       "  22,\n",
       "  5,\n",
       "  88,\n",
       "  24,\n",
       "  41,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  16,\n",
       "  220,\n",
       "  214,\n",
       "  3496,\n",
       "  7,\n",
       "  1,\n",
       "  643,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  166,\n",
       "  3,\n",
       "  599,\n",
       "  7431,\n",
       "  3,\n",
       "  25,\n",
       "  281,\n",
       "  79,\n",
       "  39816,\n",
       "  7,\n",
       "  108,\n",
       "  300,\n",
       "  49,\n",
       "  17,\n",
       "  2770,\n",
       "  3,\n",
       "  1473,\n",
       "  76,\n",
       "  41,\n",
       "  634,\n",
       "  6,\n",
       "  14,\n",
       "  104,\n",
       "  300,\n",
       "  4,\n",
       "  7431,\n",
       "  256,\n",
       "  172,\n",
       "  8,\n",
       "  32,\n",
       "  402,\n",
       "  14,\n",
       "  366,\n",
       "  4,\n",
       "  35,\n",
       "  22,\n",
       "  53,\n",
       "  184,\n",
       "  159,\n",
       "  1,\n",
       "  7381,\n",
       "  1414,\n",
       "  301,\n",
       "  73,\n",
       "  231,\n",
       "  68,\n",
       "  7,\n",
       "  100,\n",
       "  4,\n",
       "  395,\n",
       "  100,\n",
       "  60,\n",
       "  14,\n",
       "  22,\n",
       "  42,\n",
       "  56,\n",
       "  41,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  16,\n",
       "  102,\n",
       "  8,\n",
       "  32,\n",
       "  476,\n",
       "  18,\n",
       "  310,\n",
       "  33,\n",
       "  17,\n",
       "  24,\n",
       "  22,\n",
       "  422,\n",
       "  55,\n",
       "  6,\n",
       "  126,\n",
       "  25,\n",
       "  1,\n",
       "  1144,\n",
       "  3,\n",
       "  52,\n",
       "  61,\n",
       "  155,\n",
       "  1862,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  16,\n",
       "  8,\n",
       "  153,\n",
       "  124,\n",
       "  376,\n",
       "  349,\n",
       "  112,\n",
       "  18,\n",
       "  80,\n",
       "  5,\n",
       "  127,\n",
       "  25,\n",
       "  3784,\n",
       "  2174,\n",
       "  41,\n",
       "  17,\n",
       "  80,\n",
       "  634,\n",
       "  3,\n",
       "  51064,\n",
       "  3,\n",
       "  13761,\n",
       "  3,\n",
       "  31679,\n",
       "  3,\n",
       "  1981,\n",
       "  6,\n",
       "  303,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  613,\n",
       "  469,\n",
       "  6,\n",
       "  62,\n",
       "  644,\n",
       "  3,\n",
       "  1321,\n",
       "  4,\n",
       "  3663,\n",
       "  41,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  16,\n",
       "  1590,\n",
       "  17,\n",
       "  431,\n",
       "  6,\n",
       "  1196,\n",
       "  25,\n",
       "  5,\n",
       "  3223,\n",
       "  3,\n",
       "  4,\n",
       "  16,\n",
       "  198,\n",
       "  71,\n",
       "  308,\n",
       "  160,\n",
       "  41,\n",
       "  16,\n",
       "  198,\n",
       "  71,\n",
       "  247,\n",
       "  641,\n",
       "  160,\n",
       "  16,\n",
       "  7841,\n",
       "  33,\n",
       "  17,\n",
       "  1248,\n",
       "  1211,\n",
       "  469,\n",
       "  6,\n",
       "  71,\n",
       "  70,\n",
       "  41,\n",
       "  102,\n",
       "  8,\n",
       "  32,\n",
       "  117,\n",
       "  17,\n",
       "  24,\n",
       "  3,\n",
       "  52,\n",
       "  469,\n",
       "  324,\n",
       "  794,\n",
       "  6,\n",
       "  142,\n",
       "  4781,\n",
       "  70,\n",
       "  152,\n",
       "  2763,\n",
       "  159,\n",
       "  5,\n",
       "  680,\n",
       "  131,\n",
       "  14,\n",
       "  8,\n",
       "  21,\n",
       "  413,\n",
       "  52,\n",
       "  72,\n",
       "  860,\n",
       "  68,\n",
       "  1,\n",
       "  434,\n",
       "  18,\n",
       "  10,\n",
       "  2432,\n",
       "  1,\n",
       "  279,\n",
       "  41,\n",
       "  278,\n",
       "  81,\n",
       "  3,\n",
       "  31,\n",
       "  8,\n",
       "  249,\n",
       "  1359,\n",
       "  81,\n",
       "  25,\n",
       "  1,\n",
       "  1980,\n",
       "  41],\n",
       " [118,\n",
       "  118,\n",
       "  118,\n",
       "  1327,\n",
       "  118,\n",
       "  118,\n",
       "  118,\n",
       "  92,\n",
       "  34,\n",
       "  357,\n",
       "  17,\n",
       "  3,\n",
       "  57,\n",
       "  31,\n",
       "  116,\n",
       "  56,\n",
       "  165,\n",
       "  18,\n",
       "  24,\n",
       "  3,\n",
       "  276,\n",
       "  14,\n",
       "  73,\n",
       "  40,\n",
       "  5,\n",
       "  469,\n",
       "  6,\n",
       "  70,\n",
       "  2,\n",
       "  30,\n",
       "  45,\n",
       "  1,\n",
       "  109,\n",
       "  85,\n",
       "  1,\n",
       "  127,\n",
       "  10,\n",
       "  48,\n",
       "  731,\n",
       "  18,\n",
       "  14,\n",
       "  141,\n",
       "  34,\n",
       "  108,\n",
       "  113,\n",
       "  1514,\n",
       "  57,\n",
       "  31,\n",
       "  357,\n",
       "  17,\n",
       "  52,\n",
       "  34,\n",
       "  564,\n",
       "  29,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  57,\n",
       "  31,\n",
       "  36,\n",
       "  1564,\n",
       "  738,\n",
       "  7,\n",
       "  78,\n",
       "  20,\n",
       "  8681,\n",
       "  1545,\n",
       "  20,\n",
       "  52,\n",
       "  34,\n",
       "  85,\n",
       "  102,\n",
       "  8,\n",
       "  32,\n",
       "  41,\n",
       "  14,\n",
       "  8,\n",
       "  21,\n",
       "  34,\n",
       "  295,\n",
       "  366,\n",
       "  1,\n",
       "  308,\n",
       "  25,\n",
       "  1,\n",
       "  3931,\n",
       "  52,\n",
       "  1,\n",
       "  1832,\n",
       "  9,\n",
       "  279,\n",
       "  2,\n",
       "  5,\n",
       "  803,\n",
       "  20,\n",
       "  2137,\n",
       "  19,\n",
       "  246,\n",
       "  19,\n",
       "  62,\n",
       "  19,\n",
       "  505,\n",
       "  20,\n",
       "  3,\n",
       "  39,\n",
       "  110,\n",
       "  148,\n",
       "  2,\n",
       "  1,\n",
       "  127,\n",
       "  409,\n",
       "  10,\n",
       "  23,\n",
       "  2135,\n",
       "  23,\n",
       "  14,\n",
       "  63,\n",
       "  40,\n",
       "  3,\n",
       "  5,\n",
       "  659,\n",
       "  4,\n",
       "  22978,\n",
       "  336,\n",
       "  6,\n",
       "  1,\n",
       "  302,\n",
       "  982,\n",
       "  2,\n",
       "  1,\n",
       "  204,\n",
       "  62,\n",
       "  19,\n",
       "  273,\n",
       "  260,\n",
       "  49,\n",
       "  5,\n",
       "  416,\n",
       "  513,\n",
       "  1617,\n",
       "  5,\n",
       "  206,\n",
       "  1017,\n",
       "  15,\n",
       "  185,\n",
       "  861,\n",
       "  2,\n",
       "  1,\n",
       "  189,\n",
       "  1851,\n",
       "  1026,\n",
       "  6,\n",
       "  754,\n",
       "  1,\n",
       "  24,\n",
       "  113,\n",
       "  1138,\n",
       "  1948,\n",
       "  3,\n",
       "  155,\n",
       "  23,\n",
       "  1,\n",
       "  20,\n",
       "  1635,\n",
       "  20,\n",
       "  1647,\n",
       "  6,\n",
       "  1,\n",
       "  346,\n",
       "  3,\n",
       "  1,\n",
       "  20,\n",
       "  5615,\n",
       "  20,\n",
       "  6,\n",
       "  13091,\n",
       "  8,\n",
       "  21,\n",
       "  647,\n",
       "  26,\n",
       "  54,\n",
       "  1328,\n",
       "  3,\n",
       "  4,\n",
       "  48,\n",
       "  33,\n",
       "  2,\n",
       "  14248,\n",
       "  30,\n",
       "  169,\n",
       "  29,\n",
       "  523,\n",
       "  7,\n",
       "  17727,\n",
       "  1,\n",
       "  318,\n",
       "  8,\n",
       "  21,\n",
       "  2272,\n",
       "  4,\n",
       "  3912,\n",
       "  287,\n",
       "  141,\n",
       "  34,\n",
       "  38,\n",
       "  113,\n",
       "  587,\n",
       "  7,\n",
       "  3059,\n",
       "  15,\n",
       "  17,\n",
       "  619,\n",
       "  705,\n",
       "  7,\n",
       "  1,\n",
       "  88,\n",
       "  243,\n",
       "  4,\n",
       "  1,\n",
       "  2135,\n",
       "  130,\n",
       "  2,\n",
       "  277,\n",
       "  5890,\n",
       "  20806,\n",
       "  353,\n",
       "  1015,\n",
       "  15,\n",
       "  1101,\n",
       "  39,\n",
       "  6,\n",
       "  20,\n",
       "  6533,\n",
       "  8,\n",
       "  21,\n",
       "  20,\n",
       "  1134,\n",
       "  6,\n",
       "  1197,\n",
       "  15,\n",
       "  992,\n",
       "  6,\n",
       "  46,\n",
       "  318,\n",
       "  2,\n",
       "  1,\n",
       "  75,\n",
       "  62,\n",
       "  30,\n",
       "  4,\n",
       "  199,\n",
       "  171,\n",
       "  164,\n",
       "  29,\n",
       "  56,\n",
       "  20,\n",
       "  8681,\n",
       "  1545,\n",
       "  20,\n",
       "  10,\n",
       "  327,\n",
       "  6745,\n",
       "  3,\n",
       "  47,\n",
       "  3738,\n",
       "  1,\n",
       "  416,\n",
       "  1294,\n",
       "  6,\n",
       "  453,\n",
       "  6,\n",
       "  17,\n",
       "  24,\n",
       "  2,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  16,\n",
       "  22,\n",
       "  66,\n",
       "  10222,\n",
       "  3,\n",
       "  18,\n",
       "  1693,\n",
       "  12171,\n",
       "  1045,\n",
       "  17,\n",
       "  24,\n",
       "  2,\n",
       "  101,\n",
       "  20,\n",
       "  795,\n",
       "  15,\n",
       "  1811,\n",
       "  1628,\n",
       "  20,\n",
       "  4,\n",
       "  163,\n",
       "  17,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  60,\n",
       "  615,\n",
       "  7,\n",
       "  94,\n",
       "  112,\n",
       "  50,\n",
       "  20,\n",
       "  1,\n",
       "  767,\n",
       "  20,\n",
       "  4,\n",
       "  20,\n",
       "  3151,\n",
       "  912,\n",
       "  20,\n",
       "  59,\n",
       "  18,\n",
       "  22,\n",
       "  307,\n",
       "  12171,\n",
       "  533,\n",
       "  2,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  57,\n",
       "  31,\n",
       "  36,\n",
       "  273,\n",
       "  25,\n",
       "  5,\n",
       "  4156,\n",
       "  24,\n",
       "  26,\n",
       "  62,\n",
       "  273,\n",
       "  379,\n",
       "  53,\n",
       "  7,\n",
       "  38,\n",
       "  5,\n",
       "  7842,\n",
       "  2228,\n",
       "  3,\n",
       "  31,\n",
       "  158,\n",
       "  143,\n",
       "  157,\n",
       "  4,\n",
       "  78,\n",
       "  20,\n",
       "  1310,\n",
       "  8,\n",
       "  21,\n",
       "  3077,\n",
       "  20,\n",
       "  30,\n",
       "  14,\n",
       "  8,\n",
       "  21,\n",
       "  87,\n",
       "  65,\n",
       "  171,\n",
       "  3,\n",
       "  447,\n",
       "  4,\n",
       "  557,\n",
       "  19,\n",
       "  3032,\n",
       "  29,\n",
       "  319,\n",
       "  6,\n",
       "  17,\n",
       "  505,\n",
       "  2,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  9,\n",
       "  11,\n",
       "  121,\n",
       "  3575,\n",
       "  194,\n",
       "  30,\n",
       "  354,\n",
       "  55,\n",
       "  6,\n",
       "  175,\n",
       "  29,\n",
       "  2]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_reviews[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining same length sequences\n",
    "\n",
    "# Sequence Length is a Hyper-Parameter\n",
    "sequence_length = 200\n",
    "\n",
    "# Initializing sequences to zeros of dimension: (#mapped_reviews x sequence_length)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    \n",
    "    # Idea to fill each row from right to left\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the data-set into train & test\n",
    "# We simply split it into halves, as the data-set is already shuffled\n",
    "\n",
    "# Train Set\n",
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "\n",
    "# Test Set\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Function to generate batches\n",
    "# Using Generators is the most optimal way for splitting the data-set into batches\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    \n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "        \n",
    "    for i in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield(x[i:i+batch_size], y[i:i+batch_size])\n",
    "        else:\n",
    "            yield(x[i:i+batch_size])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN Class to perform sentiment analysis\n",
    "class SentiRNN(object):\n",
    "    \n",
    "    def __init__(self, n_words, seq_len=200, lstm_size=256, num_layers=1,\n",
    "                 batch_size=64, learning_rate=0.0001, embed_size=200):\n",
    "        \n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        \n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "            \n",
    "    def build(self):\n",
    "        \n",
    "        # Placeholders for input data\n",
    "        tf_x = tf.placeholder(tf.int32, shape=(self.batch_size, self.seq_len), name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
    "        \n",
    "        # Embedding is a feature-learning technique that we utilize to learn the salient \n",
    "        # features to represent the words in our data-set. This transformation is necessary\n",
    "        # as RNN's require their inputs to be vectors of continous values\n",
    "        \n",
    "        # We'll use tf.nn.embedding_lookup that maps each integer that corresponds\n",
    "        # to a unique word, to a row of the trainable matrix\n",
    "        \n",
    "        # Embedding layer to transform the input\n",
    "        embedding = tf.Variable(\n",
    "                    tf.random_uniform((self.n_words, self.embed_size), minval=-1, maxval=1),\n",
    "                                      name='embedding')\n",
    "        \n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embed_x')\n",
    "        \n",
    "        # Creating the RNN cell, applying the dropout i.e. Stacking RNN cells\n",
    "        # with dropout\n",
    "        \n",
    "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(\n",
    "                                              tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                                              output_keep_prob=tf_keepprob)\n",
    "                                              for i in range(self.num_layers)])\n",
    "        \n",
    "        \n",
    "        # Initial State of RNN\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        print(' <<< Initial State >>>', self.initial_state)\n",
    "        \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x,\n",
    "                                                           initial_state=self.initial_state)\n",
    "        \n",
    "        # lstm_outputs shape:\n",
    "        # [batch_size, max_time, cells.output_size]\n",
    "        print('\\n <<< lstm_output >>>', lstm_outputs)\n",
    "        print('\\n <<< final state >>>', self.final_state)\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[:, -1],\n",
    "                                 units=1, activation=None, name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n <<< logits >>> ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        \n",
    "        predictions = {\n",
    "                        'probabilities': y_proba,\n",
    "                        'labels': tf.cast(tf.round(y_proba), tf.int32,\n",
    "                                          name='labels')\n",
    "        }\n",
    "        \n",
    "        print('\\n <<< predictions >>>', predictions)\n",
    "        \n",
    "        # Cost Function\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, \n",
    "                                                                     logits=logits,\n",
    "                                                                     name='cost'))\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "    \n",
    "    # Training Method\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                                        X_train, y_train, self.batch_size):\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state: state}\n",
    "                    \n",
    "                    loss, _, state = sess.run(['cost:0', 'train_op',\n",
    "                                              self.final_state],\n",
    "                                              feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 5 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d\"\n",
    "                              \"| Train Loss: %.5f\" % (epoch+1, num_epochs, iteration, loss))\n",
    "                        \n",
    "                if((epoch+1)%10 == 0):\n",
    "                    # Saving our model\n",
    "                    self.saver.save(sess, \"model/sentiment-%d.ckpt\" % epoch)\n",
    "            \n",
    "    \n",
    "    # Prediction Method\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            # Restoring our model\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint('./model/'))\n",
    "            \n",
    "            test_state = sess.run(self.initial_state)\n",
    "            \n",
    "            for i, batch_x in enumerate(\n",
    "                        create_batch_generator(X_data, None, batch_size=self.batch_size)):\n",
    "                \n",
    "                feed = {'tf_x:0': batch_x,\n",
    "                        'tf_keepprob:0' : 1.0,\n",
    "                        self.initial_state : test_state}\n",
    "                \n",
    "                if(return_proba):\n",
    "                    pred, test_state = sess.run(['probabilities:0', self.final_state], \n",
    "                                                feed_dict=feed)\n",
    "                    \n",
    "                else:\n",
    "                    pred, test_state = sess.run(['labels:0', self.final_state ],\n",
    "                                                feed_dict=feed)\n",
    "                    \n",
    "                preds.append(pred)    \n",
    "        \n",
    "        return(np.concatenate(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <<< Initial State >>> (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      " <<< lstm_output >>> Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      " <<< final state >>> (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      " <<< logits >>>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      " <<< predictions >>> {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "# Because of small size of our data-set, we set num_layers=1\n",
    "\n",
    "# Single layer RNN may generalise better to unseen data, as it \n",
    "# is less likely to overfit\n",
    "rnn = SentiRNN(n_words=n_words,\n",
    "               seq_len=sequence_length,\n",
    "               embed_size=256,\n",
    "               lstm_size=128,\n",
    "               num_layers=1,\n",
    "               batch_size=100,\n",
    "               learning_rate=0.001\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training our RNN\n",
    "\n",
    "rnn.train(X_train, y_train, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/sentiment-9.ckpt\n",
      "Test Acc.: 0.840\n"
     ]
    }
   ],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "\n",
    "y_true = y_test[:len(preds)]\n",
    "\n",
    "# Test Accuracy\n",
    "print('Test Acc.: %.3f' % (np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/sentiment-9.ckpt\n",
      "Test Acc.: 0.840\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can use this for calculating accuracy\n",
    "proba = rnn.predict(X_test, return_proba= True)\n",
    "\n",
    "y_true = y_test[:len(preds)]\n",
    "\n",
    "# Test Accuracy\n",
    "print('Test Acc.: %.3f' % (np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
